{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7rc1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/techmoksha/iisc-cds-modules/blob/module-1/M1_MP1_NB_Hypothesis_Testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUUu9l_JfJ92"
      },
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "\n",
        "##  A Program by IISc and TalentSprint\n",
        "\n",
        "### Mini Project Notebook 2: Hypothesis Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL3yrUc-XrLS"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nq2_Otle4nO2"
      },
      "source": [
        "\n",
        "\n",
        "At the end of this Mini Project, you will be able to :\n",
        "\n",
        "\n",
        "* have a fair understanding of A/B Testing - a statistical way to compare two or more versions (A or B?)\n",
        "\n",
        "* determine not only which one (A or B) performs better but also understand if the difference between two of them is statistically significant\n",
        "\n",
        "* derive meaningful insights from the formula for Confidence Intervals with the t-distribution and how to adjust this formula using the Central Limit Theorem\n",
        "\n",
        "* know the dependent 3 factors (Power of the test, Significance level & Minimal Desired Effect) for having a required Sample Size\n",
        "\n",
        "* carry out Binomial Proportion Confidence Intervals, 2-sample Z-test and Chi-square. The objective of this mini project is to serve as an introductory guide to A/B testing, covering foundational concepts and methodologies.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction"
      ],
      "metadata": {
        "id": "92Ff_XtzI0f1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A/B tests are very commonly performed by data analysts and data scientists.\n",
        "\n",
        "It is a crucial technique in data-driven decision-making, yet it often lacks comprehensive exploration. This notebook aims to address this gap by providing a consolidated overview of A/B testing principles and practices.\n",
        "\n",
        "For this mini project, the exercises are framed to understand the results of an A/B test run by an e-commerce website. The goal is to work through this notebook to help the company understand if they should implement the new webpage or keep the old webpage or run the experiment longer to make their decision."
      ],
      "metadata": {
        "id": "M19d-lHWImDi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EqvTSjZZIUE"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlCSHY5_Y0wb"
      },
      "source": [
        "The dataset chosen for this experiment is the **ab_data.csv** which is publicly available on [Kaggle](https://www.kaggle.com/datasets/abdelrahmanrezk7/ab-testing-e-commerce-website)  \n",
        "\n",
        "This dataset consists of 2,94,478 records. Each record is made up of 5 fields.\n",
        "\n",
        "**For example**, Each record consists of 'user_id', 'timestamp', 'group', 'landing_page' and 'converted'.\n",
        "\n",
        "* **user_id:** A unique identifier assigned to each user (i.e., a visitor to the company's webpage) participating in the experiment.\n",
        "\n",
        "* **timestamp:** The timestamp indicating the time at which the user interacted with the webpage or was exposed to the experimental condition.\n",
        "\n",
        "* **group:** The group to which the user was assigned, typically denoted as either 'treatment' or 'control'. This field helps categorize users into different experimental conditions.\n",
        "\n",
        "* **landing_page:** Specifies the type of landing page or webpage variant that the user was directed to upon interaction. It distinguishes between different versions of the webpage used in the experiment.\n",
        "\n",
        "* **converted:** A binary indicator representing whether the user performed the desired action or conversion after interacting with the webpage. It typically indicates whether the user made a purchase, signed up for a service, or completed any other desired action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3vgcWwOF2cK"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYdLgvhhZtwA"
      },
      "source": [
        "The biggest e-commerce company called **FaceZonGoogAppFlix** approached to a **data science consulting** firm as a new client!\n",
        "\n",
        "They have a potential new webpage designed with the intention to increase their current conversion rates of 12% by 0.35% or more. With such an ambiguous task, they have full trust in the data science consulting firm to give them a recommendation whether to implement the new web page or keep the old webpage. Unfortunately they haven't built up a data science capability in their company, but they've used an external software called 'A/B Tester' for 23 days and then come back to the data science consulting firm with a dataset (ab_data.csv). Under this requirement scenario, what the **data science consulting firm** will do?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grading = 10 Points"
      ],
      "metadata": {
        "id": "HQrRWcHcSFrY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwmmvQnv3mLM"
      },
      "source": [
        "# @title Download the Dataset\n",
        "! wget -q https://cdn.exec.talentsprint.com/static/cds/content/ab_data.csv\n",
        "print(\"The datset was downloaded successfully\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part I - Probability**"
      ],
      "metadata": {
        "id": "zkN12h1bjv2z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epVoy2b_Z05e"
      },
      "source": [
        "#### Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBpCF4GlBPFL"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.stats as ss\n",
        "import statsmodels.api as sm\n",
        "import math as mt\n",
        "import itertools\n",
        "import random\n",
        "from patsy import dmatrices\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmOJDVdp9PYo"
      },
      "source": [
        "#### Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DrVCIg54LZp"
      },
      "source": [
        "# a. Read in the dataset and take a look at the top few rows here:\n",
        "# YOUR CODE HERE\n",
        "df = pd.read_csv('ab_data.csv')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# b. Use the below cell to find the number of rows in the dataset.\n",
        "# YOUR CODE HERE\n",
        "df.shape[0]"
      ],
      "metadata": {
        "id": "B9Jg6Z0KML6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "li5KS0i3pQqq"
      },
      "source": [
        "## Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Edt4IHsO4lua"
      },
      "source": [
        "### Task 1: Data Cleaning (1 point)\n",
        "\n",
        "* Check the number of unique users in the dataset\n",
        "\n",
        "* Check the proportion of users converted.\n",
        "  \n",
        "    **Hint:** query(), count()\n",
        "* Estimate how many times the new_page and treatment don't line up. Also estimate how many times the old_page and control do not match.\n",
        "\n",
        "* Display the total no. of non-line up pages\n",
        "\n",
        "* Check if any of the rows have missing values?\n",
        "\n",
        "#### **Treatment Group & Control Group**\n",
        "* **Treatment Group (New Webpage):**\n",
        "Users in this group will be exposed to the new webpage design.\n",
        "The effectiveness of the new webpage design will be measured by comparing the conversion rates of users (who actually make purchase of the company's products after visiting this new webpage) in this group to those in the control groups.\n",
        "\n",
        "* **Control Group (Old Webpage):**\n",
        "Users in this group will be shown a webpage that is already in use and has demonstrated effectiveness in terms of conversion rates. It means that users in Control Group 2 will see the same old webpage that is currently being used. This webpage has been proven to be effective in terms of converting visitors into purchasers (or customers) in the past.\n",
        "\n",
        "-- This group serves as a benchmark to evaluate whether the new webpage design outperforms the existing treatment.\n",
        "\n",
        "-- This group (Control Group) acts as a standard for comparison to see if the new webpage design performs better than the current one. We will use the conversion rates observed in Control Group to assess whether the changes made in the new webpage design lead to better results or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGXvWzHWZTUg"
      },
      "source": [
        "# c. The number of unique users in the dataset.\n",
        "# YOUR CODE HERE\n",
        "df['user_id'].unique().shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9Vj8ld6ZaIg"
      },
      "source": [
        "df.query('converted == 1')['converted'].count() / df.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-3Wh2RL4Z0t"
      },
      "source": [
        "# identify treatment does not match with new_page\n",
        "# YOUR CODE HERE\n",
        "treatment_mismatch_new_page = df.query('group == \"treatment\" and landing_page != \"new_page\"').shape[0]\n",
        "treatment_mismatch_new_page"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIFju1AqFd1t"
      },
      "source": [
        "# identify control does not match with old_page\n",
        "# YOUR CODE HERE\n",
        "control_mismatch_old_page = df.query('group == \"control\" and landing_page != \"old_page\"').shape[0]\n",
        "control_mismatch_old_page"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Total no. of non-line up\n",
        "# YOUR CODE HERE\n",
        "treatment_mismatch_new_page + control_mismatch_old_page"
      ],
      "metadata": {
        "id": "v15b7XvZPSF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnAgcxCHGywv"
      },
      "source": [
        "# Check for any missing values\n",
        "# YOUR CODE HERE\n",
        "# Axis = 1 for rows\n",
        "df.isnull().any(axis = 1).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gqq4CX4gG3zU"
      },
      "source": [
        "# Check datatype of each column\n",
        "# YOUR CODE HERE\n",
        "[print(df[c].dtype) for c in df.columns]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PVM_RBKbjZ6"
      },
      "source": [
        "### Task 2: Identify the not aligned rows (1 point)\n",
        "\n",
        "<u>**Part-2a:**</u>\n",
        "\n",
        "With the above dataset (achieved in Task-1) the requirement is to first identify the rows in that dataset where the treatment group is aligned with the new_page and where the control group is aligned with the old_page.\n",
        "\n",
        "**Hint:** It creates a new DataFrame containing these filtered rows.\n",
        "**('group == \"treatment\" and landing_page == \"new_page\"')**\n",
        "\n",
        "<u>**Part-2b:**</u>\n",
        "\n",
        "Now, with the help of the new dataset (achieved in Part-2a of Task-2), we need to identify the misaligned rows in the dataset (achieved in Task-1) where treatment is not aligned with new_page or control is not aligned with old_page\n",
        "\n",
        "This can be done by checking the values 'treatment' and 'control' under the 'group' column to ensure they do not correspond with the values 'new_page' and 'old_page' under the 'landing_page' column, respectively.\n",
        "\n",
        "For the rows where treatment is not aligned with new_page or control is not aligned with old_page, we cannot be sure if this row truly received the new or old page. Write your code to provide how we should handle these rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibXbIHHPQXZL"
      },
      "source": [
        "# Part-2a\n",
        "# create a new dataset that meets the specifications:\n",
        "# treatment is aligned with new_page or control is aligned with old_page\n",
        "# YOUR CODE HERE\n",
        "df_aligned1 = df.query('group == \"treatment\" and landing_page == \"new_page\"')\n",
        "df_aligned2 = df.query('group == \"control\" and landing_page == \"old_page\"')\n",
        "# Union of both dfs\n",
        "df_aligned = pd.concat([df_aligned1, df_aligned2], ignore_index=True)\n",
        "df_aligned"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpE9zOpnTn8n"
      },
      "source": [
        "# show the dimension of the new dataset created\n",
        "# YOUR CODE HERE\n",
        "df_aligned.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part-2b\n",
        "# Now, with the help of the new dataset (achieved above), identify the misaligned rows in the dataset (achieved in Task-1)\n",
        "# where treatment is not aligned with new_page or control is not aligned with old_page\n",
        "# YOUR CODE HERE\n",
        "# We find if all rows from main df are in aligned one and take negation to find non-aligned. Each row is converted to tuple on both dfs\n",
        "df_not_aligned = df[~df.apply(tuple, 1).isin(df_aligned.apply(tuple, 1))]\n",
        "df_not_aligned"
      ],
      "metadata": {
        "id": "ovn38S2RrSH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Double Check all of the correct rows were removed - this should be 0\n",
        "# YOUR CODE HERE\n",
        "# TODO\n",
        "df1 = df_not_aligned.query('group == \"treatment\" and landing_page == \"new_page\"')\n",
        "df2 = df_not_aligned.query('group == \"control\" and landing_page == \"old_page\"')\n",
        "df1.shape[0] + df2.shape[0]"
      ],
      "metadata": {
        "id": "NJyZYPBlTlIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xc3UOT74MM3O"
      },
      "source": [
        "### Task-3: Using the above new dataset, answer the following questions. (1.5 points)\n",
        "\n",
        "* How many unique user_ids are in the new dataset created above in Task-2?\n",
        "\n",
        "* There is one user_id repeated in this dataset. What is it? (Here you need to show only the user_id)\n",
        "\n",
        "* What is the row information for the repeat user_id? (Here, you need to show the complete row including 'user_id', 'timestamp', 'group', 'landing_page' and\t'converted')\n",
        "\n",
        "* Remove one of the rows with a duplicate user_id, but keep your dataframe name as same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dn_ljWWAcNM"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "df_aligned['user_id'].unique().shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the duplicated user_id\n",
        "# YOUR CODE HERE\n",
        "df_duplicated_user_id = df_aligned[df_aligned['user_id'].duplicated()]\n",
        "print(df_duplicated_user_id['user_id'].values)"
      ],
      "metadata": {
        "id": "a5feaxXUbnIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show the row information of the duplicate user_id\n",
        "# YOUR CODE HERE\n",
        "df_duplicated_user_id"
      ],
      "metadata": {
        "id": "-7MFS8RWbW7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop the duplicate user_id, keep your dataframe name as same.\n",
        "# YOUR CODE HERE\n",
        "df_aligned = df_aligned.drop_duplicates(subset='user_id', keep='first')\n",
        "df_aligned"
      ],
      "metadata": {
        "id": "558Lw12Acf8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrPzlHc4-zIR"
      },
      "source": [
        "## Finding Probabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFgtC_jCpJL1"
      },
      "source": [
        "### Task 4: After removing the duplicated user_id, answer the following: (1.5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QQ2WUQX9XYy"
      },
      "source": [
        "##### Exercise 1: What is the probability of an individual converting regardless of the page they receive?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcRjzr9YRo72"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "P_individual_converting = (df_aligned[df_aligned['converted'] == 1]).shape[0] / df_aligned.shape[0]\n",
        "round(P_individual_converting, 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzzEnMv25vGn"
      },
      "source": [
        "##### Exercise 2: Given that an individual was in the control group, what is the probability they converted?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWHrrw2BJrb7"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "df_control = df_aligned[df_aligned['group'] == 'control']\n",
        "P_individual_converting_in_control = (df_control[df_control['converted'] == 1]).shape[0] / df_control.shape[0]\n",
        "round(P_individual_converting_in_control, 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFNIQ4dj59Ep"
      },
      "source": [
        "##### Exercise 3: Given that an individual was in the treatment group, what is the probability they converted?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZDWxb_JRtBl"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "df_treatment = df_aligned[df_aligned['group'] == 'treatment']\n",
        "P_individual_converting_in_treatment = (df_treatment[df_treatment['converted'] == 1]).shape[0] / df_treatment.shape[0]\n",
        "round(P_individual_converting_in_treatment, 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBreBsQ7Vqi0"
      },
      "source": [
        "\n",
        "##### Exercise 4: What is the probability that an individual received the new page?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFUmNVsCTR_t"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "P_individual_new_page = (df_aligned[df_aligned['landing_page'] == 'new_page']).shape[0] / df_aligned.shape[0]\n",
        "round(P_individual_new_page, 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDMFaGnT8NsJ"
      },
      "source": [
        "# What is the probability that an individual received the old page?\n",
        "# YOUR CODE HERE\n",
        "# P_individual_old_page = (df_aligned[df_aligned['landing_page'] == 'old_page']).shape[0] / df_aligned.shape[0]\n",
        "# round(P_individual_old_page, 4)\n",
        "P_individual_old_page = 1 - P_individual_new_page\n",
        "round(P_individual_old_page, 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YKbXa-R8dCO"
      },
      "source": [
        "# show the difference: (treatment_convert - control_convert)\n",
        "# YOUR CODE HERE\n",
        "# TODO?\n",
        "round(round(P_individual_converting_in_control, 4) - round(P_individual_converting_in_treatment, 4), 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiOWt855DsZ8"
      },
      "source": [
        "##### Exercise 5: Use the results in the previous two portions of this question to suggest if you think there is evidence that one page leads to more conversions? Write your response below."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Write your response here:**\n",
        "\n"
      ],
      "metadata": {
        "id": "vr5JV56sf5ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part II - A/B Test**"
      ],
      "metadata": {
        "id": "FpqFMlUxkuMR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that because of the time stamp associated with each event, you could technically run a hypothesis test continuously as each observation was observed.\n",
        "\n",
        "However, then the hard question is do you stop as soon as one page is considered significantly better than another or does it need to happen consistently for a certain amount of time? How long do you run to render a decision that neither page is better than another?\n",
        "\n",
        "These questions are the difficult parts associated with A/B tests in general.\n",
        "\n",
        "1. For now, consider you need to make the decision just based on all the data provided. If you want to assume that the old page is better unless the new page proves to be definitely better at a Type I error rate of 5% (i.e. 0.05), what should your null and alternative hypotheses be? You can state your hypothesis in terms of words or in terms of $p_{old}$ and $p_{new}$, which are the converted rates for the old and new pages.\n",
        "\n",
        "$$  H_{null} : p_{new} - p_{old} <= 0 $$  \n",
        "\n",
        "$$  H_{alternative} : p_{new} - p_{old} > 0 $$\n",
        "\n",
        "2. Assume under the null hypothesis, $p_{new}$ and $p_{old}$ both have \"true\" success rates equal to the converted success rate regardless of page - that is $p_{new}$ and $p_{old}$ are equal. Furthermore, assume they are equal to the converted rate in ab_data.csv regardless of the page.\n",
        "\n",
        "Use a sample size for each page equal to the ones in the new dataset achieved at the end of Task-3.\n",
        "\n",
        "Perform the sampling distribution in the following exercises under Task-5 for the difference in converted between the two pages over 10,000 iterations of calculating an estimate from the null."
      ],
      "metadata": {
        "id": "ky7IFjnNlDVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "zEG31avWsX2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boBhK2SdGXlW"
      },
      "source": [
        "### Task 5: Derive the below insights (2 points)\n",
        "\n",
        "* **$n_{old}$** - The number of individuals (each having unique user_id) who received the old page (control group)\n",
        "\n",
        "* **$n_{new}$** - The Number of individuals (each having unique user_id) who received the new page (treatment group)\n",
        "\n",
        "* **$p_{new}$ under the null:** This refers to the conversion rate for the new page (treatment group) that we would expect to observe if the null hypothesis ($H_{null}$) is true.\n",
        "In other words, it represents the probability of conversion for users who are exposed to the new page under the assumption that there is no difference in conversion rates between the old and new pages.\n",
        "\n",
        "So, basically **$p_{new}$ under the null** is the conversion rate for the new page that we would expect to see if the new page had no impact on user behavior compared to the old page. It serves as a reference point for evaluating the observed conversion rate for the new page in relation to the null hypothesis ($H_{null}$).\n",
        "\n",
        "* **$p_{old}$ under the null:** This refers to the conversion rate for the old page (control group) that we would expect to observe if the null hypothesis ($H_{null}$) is true.\n",
        "In other words, it represents the probability of conversion for users who are exposed to the old page under the assumption that there is no difference in conversion rates between the old and new pages.\n",
        "\n",
        "So, basically **$p_{old}$ under the null** is the conversion rate for the old page that we would expect to see if the old page had no impact on user behavior compared to the new page. It serves as a reference point for evaluating the observed conversion rate for the old page in relation to the null hypothesis ($H_{null}$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtVLkGB_ANwf"
      },
      "source": [
        "##### Exercise 1: What is the convert rate for $p_{new}$ under the null?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "p_new = round(P_individual_converting_in_treatment, 4)\n",
        "p_new"
      ],
      "metadata": {
        "id": "9vZnqsXZxzdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb861WejndKI"
      },
      "source": [
        "##### Exercise 2: What is the convert rate for $p_{old}$ under the null?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWftl4eC2jNU"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "p_old = round(P_individual_converting_in_control, 4)\n",
        "p_old"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kksy2sD4CMKQ"
      },
      "source": [
        "##### Exercise 3: What is $n_{new}$?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZpnlYfHCO3P"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "n_new = df_treatment.shape[0]\n",
        "n_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnhRhfWadnGK"
      },
      "source": [
        "##### Exercise 4: What is $n_{old}$?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACEuvXBApg1y"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "n_old = df_control.shape[0]\n",
        "n_old"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "this one was tricky. But here we are looking at a null where there is no difference in conversion based on the page, which means the conversions for each page are the same."
      ],
      "metadata": {
        "id": "KpJ67AZKuJl3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSCEb0GX5-d1"
      },
      "source": [
        "##### Exercise 5: Simulate $n_{new}$ transactions with a convert rate of $p_{new}$ under the null. Store these $n_{new}$ 1's and 0's in **new_page_converted**.\n",
        "\n",
        "**Hint:** Use **np.random.binomial()**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8kGpmMmx7HI"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "new_page_converted = np.random.binomial(n_new, p_new)\n",
        "new_page_converted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXQfqIixzC4_"
      },
      "source": [
        "##### Exercise 6: Simulate $n_{old}$ transactions with a convert rate of $p_{old}$ under the null. Store these $n_{old}$ 1's and 0's in **old_page_converted**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XB7M6Sx74bFU"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "old_page_converted = np.random.binomial(n_old, p_old)\n",
        "old_page_converted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Exercise 7: Find $(p_{new} - p_{old})$ for your simulated values from Exercise 5 and Exercise 6."
      ],
      "metadata": {
        "id": "OGMPVx-oLVDM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCqAHD0OLyFj"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "p_new_sample = new_page_converted / n_new\n",
        "round(p_new_sample, 4)\n",
        "\n",
        "p_old_sample = old_page_converted / n_old\n",
        "round(p_old_sample, 4)\n",
        "\n",
        "print(round(p_new_sample - p_old_sample, 4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Exercise 8: Simulate 10,000 $(p_{new}-p_{old})$ values using this same process similarly to the one you calculated in Exercise 1 through Exercise 7 above. Store all 10,000 values in $p_{diffs}$."
      ],
      "metadata": {
        "id": "yTwkV1aqJ2VL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "new_page_converted_simulation = np.random.binomial(n_new, p_new, 10000) / n_new\n",
        "old_page_converted_simulation = np.random.binomial(n_old, p_old, 10000) / n_old\n",
        "p_diffs = new_page_converted_simulation - old_page_converted_simulation\n",
        "p_diffs"
      ],
      "metadata": {
        "id": "ocQ4FNDHxmKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, we can do the same approach using the following snippet in order to eliminate the use of for:"
      ],
      "metadata": {
        "id": "iSGLfvkPOU0V"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jb8t1F_DvgpD"
      },
      "source": [
        "new_converted_simulation = np.random.binomial(n_new, p_new,  10000)/n_new\n",
        "old_converted_simulation = np.random.binomial(n_old, p_old,  10000)/n_old\n",
        "new_converted_simulation - old_converted_simulation\n",
        "# Essentially, we are applying the null proportion to the total size of each page using the binomial distribution.\n",
        "# Each element, for example, in np.random.binomial(n_new, p_new, 10000) results in an array with values like [17262, 17250, 17277...].\n",
        "# This array is 10000 elements large When we divide it by n_new, Python broadcasts n_new for each element and we return a proportion for each element.\n",
        "# This is essentially is simulating, 10000, the new page conversion rate."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Exercise 9: Plot a histogram of the $p_{diffs}$. Does this plot look like what you expected? Use the matching problem to assure your understanding on what was computed here."
      ],
      "metadata": {
        "id": "uzh2XHhV2Rrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "plt.hist(p_diffs, bins=100, edgecolor='black')\n",
        "\n",
        "# Add labels and title\n",
        "plt.title('Histogram of P Diffs')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QpZqLPFO2Z1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes the difference in probabilities of the 10000 samples follows approximately normal distribution."
      ],
      "metadata": {
        "id": "6fxj5aO677be"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save p_diffs for later use\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "N3ZTwlOQQLil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sample size of df2 is large enough that our sampling distribution is bell shaped. As we observed the actual difference in $(p_{new}-p_{old})$, based on the confidence interval, we have an equal difference in means between old and new pages. The normal distribution is small, that is, it's between (-0.05 and +0.05)."
      ],
      "metadata": {
        "id": "6bA5W27K3HVl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Exercise 10: What proportion of the $p_{diffs}$ are greater than the actual difference observed in ab_data.csv?"
      ],
      "metadata": {
        "id": "e_BsdWfT0mxu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The corresponding Z Score is\n",
        "$$z = \\frac{({p_{new}} - {p_{old}})}{\\sqrt{SE \\big(P_{new} - P_{old}\\big)}}$$\n",
        "\n",
        "Standard error given by\n",
        "$SE = \\frac{(p) * (1 - p)}{\\frac{1}{n_{new}} + \\frac{1}{n_{old}}}$"
      ],
      "metadata": {
        "id": "EXNcgpYuWhzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "# Calculate Z score with (p_new - p_old) / SE(P_diffs)\n",
        "total_conv_prob = (new_page_converted + old_page_converted) / (n_new + n_old)\n",
        "se_denom = (1 / n_new) + (1 / n_old)\n",
        "standard_error = np.sqrt(total_conv_prob * (1 - total_conv_prob) * se_denom)\n",
        "\n",
        "z_score = (p_new - p_old) / standard_error\n",
        "z_score"
      ],
      "metadata": {
        "id": "lC_zA7oV2g5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Exercise 11: In words, explain what you just computed in Exercise 10.\n",
        "What is this value called in scientific studies?\n",
        "\n",
        "What does this value mean in terms of whether or not there is a difference between the new and old pages?"
      ],
      "metadata": {
        "id": "OxDUjLF01CoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What is this value called in scientific studies?\n",
        "# Simulate distribution under the null hypothesis\n",
        "# YOUR CODE HERE\n",
        "# mean=0, std=1 for the null distribution\n",
        "pdf = norm.pdf(x, 0, 1)\n",
        "\n",
        "# Plot the null distribution\n",
        "# YOUR CODE HERE\n",
        "x = np.linspace(-3, 3, 1000)  # range from -3 to 3, since 95% of data lies within this range\n",
        "\n",
        "# Plot the null distribution (standard normal distribution)\n",
        "plt.plot(x, pdf, color=\"blue\")\n",
        "plt.axvline(z_score, color=\"red\", linestyle=\"--\", label=f\"Z-Score = {z_score}\")\n",
        "\n",
        "# Fill the area under the curve beyond the z-score (right tail for a right-tailed test)\n",
        "plt.fill_between(x, 0, pdf, where=(x >= z_score), color=\"red\", alpha=0.05, label=f\"Area Beyond Z = {z_score}\")\n",
        "\n",
        "# Labels and title\n",
        "plt.title(f\"Null Distribution with Z-Score = {z_score}\")\n",
        "plt.xlabel(\"Z-Score\")\n",
        "plt.ylabel(\"Probability Density\")\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1KEb5c_g1Vi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute p-value (consider h_alternative : p_new > p_old)\n",
        "# YOUR CODE HERE\n",
        "# p_value = P(Z > -1.32)\n",
        "p_value = 1 - norm.cdf(z_score)\n",
        "p_value"
      ],
      "metadata": {
        "id": "_V7wEymF1ba6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The actual difference is captured in the population. Since $p_{value}>0.05$, we would fail to reject the null hypothesis.**\n",
        "\n",
        "100% of values from our null distribution fall to the right our actual difference. The old page has a higher probability of convertion rate than the new page."
      ],
      "metadata": {
        "id": "YGc0JnGL1p_b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Exercise 12: We could also use a built-in to achieve similar results. Though using the built-in might be easier to code, the above portions are a walkthrough of the ideas that are critical to correctly thinking about statistical significance.\n",
        "\n",
        "Write your code in the below cells to calculate the number of conversions for each page, as well as the number of individuals who received each page. Let $n_{old}$ and $n_{new}$ refer the the number of rows associated with the old page and new pages, respectively."
      ],
      "metadata": {
        "id": "WlGsRIBN2orT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the number of conversions for each page\n",
        "# YOUR CODE HERE\n",
        "n_old_conv = df_control[df_control['converted'] == 1].shape[0]\n",
        "n_new_conv = df_treatment[df_treatment['converted'] == 1].shape[0]\n",
        "\n",
        "\n",
        "# the number of individuals who received each page\n",
        "# YOUR CODE HERE\n",
        "n_old = df_control.shape[0]\n",
        "n_new = df_treatment.shape[0]"
      ],
      "metadata": {
        "id": "3DdLn5Zn3HYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Exercise 13: Now use stats.proportions_ztest to compute your test statistic and p-value. Here is a helpful [link](https://www.statsmodels.org/stable/generated/statsmodels.stats.proportion.proportions_ztest.html) on using the built in."
      ],
      "metadata": {
        "id": "emh0ZGtI3VBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "count = np.array([n_new_conv, n_old_conv])  # successes in both samples\n",
        "nobs = np.array([n_new, n_old])  # sample sizes for both samples\n",
        "# Since Alternative hypothesis asks for pnew > pold we look for right tailed test\n",
        "pair_ztest = sm.stats.proportions_ztest(count, nobs, 0, 'larger')"
      ],
      "metadata": {
        "id": "6F3C6J7j3cBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the z_score and p_value\n",
        "# YOUR CODE HERE\n",
        "print(\"test-statistic (z-score): \", pair_ztest[0])\n",
        "# YOUR CODE HERE to show p-value for pair ztest\n",
        "print(\"p-value: \", round(pair_ztest[1], 4))"
      ],
      "metadata": {
        "id": "23bgRiX03iYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import norm\n",
        "# Tells us how significant our z-score is?\n",
        "# YOUR CODE HERE\n",
        "# For a one-tailed test (right-tailed):\n",
        "p_value_one_tailed_right = 1 - norm.cdf(pair_ztest[0])\n",
        "round(p_value_one_tailed_right, 4)"
      ],
      "metadata": {
        "id": "P3FECBkC3n62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tells us what our critical value at 95% confidence is?\n",
        "# YOUR CODE HERE\n",
        "alpha = 1 - 0.95\n",
        "z_critical = norm.ppf(1 - alpha)\n",
        "round(z_critical, 4)"
      ],
      "metadata": {
        "id": "Djj5F6on3xhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Exercise 14: What do the z-score and p-value you computed in the previous question mean for the conversion rates of the old and new pages? Do they agree with the findings in parts Exercise 10 and Exercise 11?"
      ],
      "metadata": {
        "id": "_fLt4UYq3_4z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Your response here:**"
      ],
      "metadata": {
        "id": "rd_ZfMI_4ZRd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We would view various methods to conclude if $p_{new} = p_{old}$ or $p_{new} > p_{old}$:**"
      ],
      "metadata": {
        "id": "P5KSs833vhEY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Method-1: Binomial Proportion Confidence Intervals**\n",
        "\n",
        "This method is quoted as the 'most common' method for A/B testing, where we find Confidence Intervals (CI) for both $p_{new}$ and $p_{old}$. If we construct similar intervals for both and compare them, we will end up in either scenario:\n",
        "\n",
        "1. The Intervals do not overlap: This implies that we can say with some level of confidence that one is better than the other, therefore providing enough evidence to reject the Null Hypothesis. This level of confidence seems to be $≈ 1-e \\alpha^{1.91}%$ (Lan, 2011). So if there is overlap and the 95% CI are the same size, the difference is significant at the 99.5% level.\n",
        "\n",
        "2. The Intervals do overlap: Then it is either a sign that our population does not have enough statistical power, or we do not have enough evidence to reject the Null Hypothesis that $p_{new} = p_{old}$.\n",
        "\n",
        "There is a relationship between CI comparisons and hypothesis tests - given that the sample sizes are not too different and the two sets have similar standard deviations.\n",
        "\n",
        "Finding the 'true' conversion rate of a particular group is usually impossible or difficult, but we can use our calculated $p_{new}$ and $p_{old}$ as point estimations to find the Confidence Intervals for the 'true' $p_{new}$ and $p_{old}$.\n",
        "\n",
        "In this context we define $CI_{new}$ and $CI_{old}$ as below.\n",
        "* **$CI_{new}$:** The confidence interval $CI_{new}$ represents the range of values within which we estimate the true conversion rate $p_{new}$ of the new group lies with 95% confidence.\n",
        "\n",
        "* **$CI_{old}$:** The confidence interval $CI_{old}$ represents the range of values within which we estimate the true conversion rate $p_{old}$ of the old group lies with 95% confidence."
      ],
      "metadata": {
        "id": "EpXGsGnwv2QT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 1: Define a customized function **get_z_score()** to get the z-score. Consider $\\alpha = 5$% (0.5 point)\n",
        "\n",
        "**Hint:** You can use the inverse cumulative distribution function (CDF) of the standard normal distribution [i.e., scipy.stats.norm.ppf() in Python] to find the z-score"
      ],
      "metadata": {
        "id": "X8W59XZVA34I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function for getting z-scores for alpha. For our experiemnt where alpha = 5%, keep in mind we want to input 1-alpha/2 for Confidence Intervals.\n",
        "# YOUR CODE HERE\n",
        "def get_z_score(alpha: float) -> float:\n",
        "    return norm.ppf(1 - alpha / 2)\n",
        "\n",
        "get_z_score(0.05)"
      ],
      "metadata": {
        "id": "P1nRYPOTzU2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 2 **(Optional)**: Now calculate the $CI_{old}$ and $CI_{new}$ using the **get_z_score** and given $\\alpha = 5$% (0.5 point)"
      ],
      "metadata": {
        "id": "MeYhtT7tBEDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check whether Confidence Interval old (CI_old) & Confidence Interval new (CI_new) overlap?\n",
        "# YOUR CODE HERE\n",
        "def get_mean_value_ci(values, sample_mean) -> tuple:\n",
        "    # Calculate std deviation\n",
        "    std_dev = np.std(values)\n",
        "    true_mean_0 = sample_mean - (get_z_score(0.05) * std_dev / np.sqrt(len(values)))\n",
        "    true_mean_1 = sample_mean + (get_z_score(0.05) * std_dev / np.sqrt(len(values)))\n",
        "    return (round(true_mean_0, 4), round(true_mean_1, 4))"
      ],
      "metadata": {
        "id": "eC8GheOpybca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ci_old = get_mean_value_ci(df_control['converted'], p_old)\n",
        "ci_new = get_mean_value_ci(df_treatment['converted'], p_new)\n",
        "ci_old, ci_new"
      ],
      "metadata": {
        "id": "1oAvIKbAufPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both CI intervals overlap plenty as $CI_{new}$ is completely contained within $CI_{old}$ , which means we do not reject the Null Hypothesis that $p_{new} = p_{old}$.\n",
        "\n",
        "    This means that the new page is not better than the old page.\n",
        "\n",
        "While our case is quite evident that the overlap is significantly clear, slight overlaps could tempt us to draw the same conclusion to reject the Null Hypothesis. However, this is a common misinterpretation of overlapping CIs when comparing groups. Failure to do so could result in incorrect or misleading conclusions being drawn (Tan & Tan, 2010, pp. 278)."
      ],
      "metadata": {
        "id": "Rnsmj5hpzwdu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Method-2: Z-test**\n",
        "\n",
        "We can use existing packages to calculate our test statistic and p-values and test for proportions based on the **z-test.** This is similar to the Binomial Proportion Confidence Interval Test, is quantitatively easier to draw conclusions out of due to it returning a p-value:"
      ],
      "metadata": {
        "id": "queM7QwO0HbI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 1: Calculate the total number of conversions for each group (convert_old & convert_new) (0.5 point)\n",
        "\n",
        "**Hints:**\n",
        "* Use the condition (\"landing_page == 'old_page' and converted == 1\") for calculating convert_old\n",
        "* Use the condition (\"landing_page == 'new_page' and converted == 1\") for calculating convert_new"
      ],
      "metadata": {
        "id": "chYlkX-46_4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#returning the total number of conversions for each group: (convert_old & convert_new)\n",
        "# YOUR CODE HERE\n",
        "convert_old = df.query('landing_page == \"old_page\" and converted == 1')['converted']\n",
        "convert_new = df.query('landing_page == \"new_page\" and converted == 1')['converted']"
      ],
      "metadata": {
        "id": "3sWF5r9s0e1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convert_new.shape[0]"
      ],
      "metadata": {
        "id": "ovxLHrvwxUln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 2: Calculate the z_score and p_value using the one-sided z-test (0.5 point)"
      ],
      "metadata": {
        "id": "Rslaarjv7T6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#calculating the z-score + p-value using the z-test (one-sided):\n",
        "# YOUR CODE HERE\n",
        "count = np.array([convert_new.shape[0], convert_old.shape[0]])  # successes in both samples\n",
        "nobs = np.array([df_treatment.shape[0], df_control.shape[0]])  # sample sizes for both samples\n",
        "# Since Alternative hypothesis asks for pnew > pold we look for right tailed test\n",
        "two_sample_ztest, p_value = sm.stats.proportions_ztest(count, nobs, 0, 'larger')\n",
        "p_value"
      ],
      "metadata": {
        "id": "pTimrb7E0j5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given our **p-value ≈ 0.9 > 0.05**, we do not reject the Null Hypothesis.\n",
        "\n",
        "    This means that the new page is not better than the old page."
      ],
      "metadata": {
        "id": "tzDh0qAm0u_A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Method-3: Chi-Squared Test**\n",
        "\n",
        "One statistical test that came out is the Chi-Squared Analysis (or $\\chi^{2}$ test). If we constructed a 2x2 contingency table for our observed frequencies in our dataset, and compared it to a 2x2 contingency table for the expected frequencies in our dataset, we can perform the $\\chi^{2}$ test under the Null Hypothesis that there is no relationship that exists on between our conversion vs their treatment/control group in the population.\n",
        "\n",
        "For reference, our 2x2 contingency table will have two groups: treatment/control or converted/not converted. We want to make 4 calculations that will be in our table:\n",
        "1. Treatment, converted\n",
        "2. Treatment, not converted\n",
        "3. Control, converted\n",
        "4. Control, not converted"
      ],
      "metadata": {
        "id": "RiZV_X-u60PW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 1: (0.5 point)\n",
        "* Calculate the 4 entities (treatment_converted, treatment_not_converted, control_converted & control_not_converted)\n",
        "* Create the 2x2 Contingency table which will be required to do the Chi-Square test"
      ],
      "metadata": {
        "id": "wajBEx9lFr4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Do the 4 calculations as above:\n",
        "# YOUR CODE HERE\n",
        "treatment_converted = df_treatment[df_treatment['converted'] == 1].shape[0]\n",
        "treatment_not_converted = df_treatment[df_treatment['converted'] == 0].shape[0]\n",
        "\n",
        "control_converted = df_control[df_control['converted'] == 1].shape[0]\n",
        "control_not_converted = df_control[df_control['converted'] == 0].shape[0]\n",
        "\n",
        "# # create the array to do our chi-squared test: treatment/control along the rows and converted/not converted along the columns:\n",
        "# # YOUR CODE HERE\n",
        "cont_table = np.array([[treatment_converted, treatment_not_converted], [control_converted, control_not_converted]])\n",
        "cont_table"
      ],
      "metadata": {
        "id": "kxf3iYzg7RrV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a444a26-ba03-4085-fee1-3b5055a6fc17"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 17264, 128046],\n",
              "       [ 17489, 127785]])"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 2: Carry out the Chi-Square test and estimate the p-value (0.5 point)"
      ],
      "metadata": {
        "id": "tHYO7N2ZFwts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using scipy stats to perform our chi squared test:\n",
        "# YOUR CODE HERE\n",
        "total_converted = sum(cont_table[:,0])\n",
        "total_not_converted = sum(cont_table[:,1])\n",
        "\n",
        "total_treatment = sum(cont_table[0])\n",
        "total_control = sum(cont_table[1])\n",
        "\n",
        "total_results = total_treatment + total_control\n",
        "\n",
        "expected_values = np.array([[total_converted * total_treatment / total_results, total_not_converted * total_treatment / total_results],\n",
        "                            [total_converted * total_control / total_results, total_not_converted * total_control / total_results]])"
      ],
      "metadata": {
        "id": "M41w8pql8fJU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25839154-1b31-44a8-b40a-ef643d75654a"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 17378.65274757, 127931.34725243],\n",
              "       [ 17374.34725243, 127899.65274757]])"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import chi2_contingency\n",
        "chi2_stat, p_val, dof, expected = chi2_contingency(cont_table)\n",
        "print(f\"Chi-Squared Statistic: {chi2_stat}\")\n",
        "print(f\"P-value: {p_val}\")\n",
        "# print(f\"Expected: {expected}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3Rw2oVRF6LU",
        "outputId": "c2ff8672-2871-4aa6-ba12-bb4162fef379"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chi-Squared Statistic: 1.7035660051885055\n",
            "P-value: 0.1918222809623566\n",
            "Expected: [[ 17378.65274757 127931.34725243]\n",
            " [ 17374.34725243 127899.65274757]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Give your remarks here on the results achieved from Chi-Square Test.**"
      ],
      "metadata": {
        "id": "uuczNsqz8lsr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since P > 0.05, we retain null hypothesis."
      ],
      "metadata": {
        "id": "5Yf_jThDG-3Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion:**\n",
        "\n",
        "The A/B testing experiment was designed to determine if <b>FaceZonGoogAppFlix</b>'s new webpage would improve the conversion rate of their users compared to their existing one.\n",
        "\n",
        "After going through multiple statistical methods to determine a winner of the A/B test, We've seen that <b>FaceZonGoogAppFlix</b>'s underlying goal had not been reached with their new webpage.\n",
        "\n",
        "Hence, we recommend that to not continue with the new webpage change, but pursue other strategies & experiments."
      ],
      "metadata": {
        "id": "_59--Mj28_lu"
      }
    }
  ]
}