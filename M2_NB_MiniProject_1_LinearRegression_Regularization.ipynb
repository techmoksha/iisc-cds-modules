{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/techmoksha/iisc-cds-modules/blob/module-1/M2_NB_MiniProject_1_LinearRegression_Regularization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chief-journalist"
      },
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "## A program by IISc and TalentSprint\n",
        "### Mini-Project: Linear Regression with Regularization"
      ],
      "id": "chief-journalist"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arabic-shaft"
      },
      "source": [
        "## Problem Statement"
      ],
      "id": "arabic-shaft"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prospective-thread"
      },
      "source": [
        "Predict the bike-sharing counts per hour based on the features including weather, day, time, humidity, wind speed, season e.t.c."
      ],
      "id": "prospective-thread"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "organic-christmas"
      },
      "source": [
        "## Learning Objectives"
      ],
      "id": "organic-christmas"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phantom-begin"
      },
      "source": [
        "At the end of the mini-project, you will be able to :\n",
        "\n",
        "* perform data exploration and visualization\n",
        "* implement linear regression using sklearn and optimization\n",
        "* apply regularization on regression using Lasso, Ridge and Elasticnet techniques\n",
        "* calculate and compare the MSE value of each regression technique\n",
        "* analyze the features that are best contributing to the target"
      ],
      "id": "phantom-begin"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "received-quilt"
      },
      "source": [
        "### Dataset"
      ],
      "id": "received-quilt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sporting-replication"
      },
      "source": [
        "The dataset chosen for this mini-project is [Bike Sharing Dataset](https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset).  This dataset contains the hourly and daily count of rental bikes between the years 2011 and 2012 in the capital bike share system with the corresponding weather and seasonal information. This dataset consists of 17389 instances of 16 features.\n",
        "\n",
        "Bike sharing systems are a new generation of traditional bike rentals where the whole process from membership, rental and return has become automatic. Through these systems, the user can easily rent a bike from a particular position and return to another position. Currently, there are about over 500 bike-sharing programs around the world which is composed of over 500 thousand bicycles. Today, there exists great interest in these systems due to their important role in traffic, environmental and health issues.\n",
        "\n",
        "Apart from interesting real world applications of bike sharing systems, the characteristics of data being generated by these systems make them attractive for the research. As opposed to other transport services such as bus or subway, the duration of travel, departure and arrival position are explicitly recorded in these systems. This feature turns bike sharing system into a virtual sensor network that can be used for sensing mobility in the city. Hence, it is expected that the most important events in the city could be detected via monitoring these data."
      ],
      "id": "sporting-replication"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ISKMyTABbfp"
      },
      "source": [
        "<img src=\"https://s26551.pcdn.co/wp-content/uploads/2012/02/resize-va-sq-bikeshare.jpg\" alt=\"drawing\" width=\"400\"/>"
      ],
      "id": "5ISKMyTABbfp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "binary-evening"
      },
      "source": [
        "### Data Fields\n",
        "\n",
        "* dteday - hourly date\n",
        "* season - 1:winter, 2:spring, 3:summer, 4:fall\n",
        "* hr - hour\n",
        "* holiday - whether the day is considered a holiday\n",
        "* workingday - whether the day is neither a weekend nor holiday\n",
        "* weathersit -<br>\n",
        "    1 - Clear, Few clouds, Partly cloudy, Partly cloudy <br>\n",
        "    2 - Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist<br>\n",
        "    3 - Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds<br>\n",
        "    4 - Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog<br>   \n",
        "* temp - temperature in Celsius\n",
        "* atemp - \"feels like\" temperature in Celsius\n",
        "* humidity - relative humidity\n",
        "* windspeed - wind speed\n",
        "* casual - number of non-registered user rentals initiated\n",
        "* registered - number of registered user rentals initiated\n",
        "* cnt - number of total rentals"
      ],
      "id": "binary-evening"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "perfect-fields"
      },
      "source": [
        "## Information"
      ],
      "id": "perfect-fields"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quick-gasoline"
      },
      "source": [
        "**Regularization:** It is a form of regression that shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model, to avoid the risk of overfitting. A simple relation for linear regression looks like this.\n",
        "\n",
        "$Y ≈ β_0 + β_1 X_1 + β_2 X_2 + …+ β_p X_p$\n",
        "\n",
        " Here $Y$ represents the learned relation and $β$ represents the coefficient estimates for different variables or predictors(X).\n",
        "\n",
        " If there is noise in the training data, then the estimated coefficients won’t generalize well to the future data. This is where regularization comes in and shrinks or regularizes these learned estimates towards zero.\n",
        "\n",
        "Below are the Regularization techniques:\n",
        "\n",
        " * Ridge Regression\n",
        " * Lasso Regression\n",
        " * Elasticnet Regression"
      ],
      "id": "quick-gasoline"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "declared-battle"
      },
      "source": [
        "## Grading = 10 Points"
      ],
      "id": "declared-battle"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "virtual-ownership"
      },
      "source": [
        "#@title Download the dataset\n",
        "!wget -qq https://cdn.iisc.talentsprint.com/CDS/MiniProjects/Bike_Sharing_Dataset.zip\n",
        "!unzip Bike_Sharing_Dataset.zip"
      ],
      "id": "virtual-ownership",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cross-survivor"
      },
      "source": [
        "#### Importing Necessary Packages"
      ],
      "id": "cross-survivor"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ethical-essence"
      },
      "source": [
        "# Loading the Required Packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
      ],
      "id": "ethical-essence",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "through-scotland"
      },
      "source": [
        "### Data Loading"
      ],
      "id": "through-scotland"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "comic-consolidation"
      },
      "source": [
        "# Read the hour.csv file\n",
        "# YOUR CODE HERE\n",
        "df = pd.read_csv('hour.csv')\n",
        "df"
      ],
      "id": "comic-consolidation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ordered-overall"
      },
      "source": [
        "print the first five rows of dataset"
      ],
      "id": "ordered-overall"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exterior-committee"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "df.head()"
      ],
      "id": "exterior-committee",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e67gbVvLIsoS"
      },
      "source": [
        "print the datatypes of the columns"
      ],
      "id": "e67gbVvLIsoS"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sharp-shelter"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "df.dtypes"
      ],
      "id": "sharp-shelter",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['hr'].unique().shape[0]"
      ],
      "metadata": {
        "id": "IH3F8_tqWGc7"
      },
      "id": "IH3F8_tqWGc7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opponent-block"
      },
      "source": [
        "### Task flow with respect to feature processing and model training\n",
        "\n",
        "* Explore and analyze the data\n",
        "\n",
        "* Identify continuous features and categorical features\n",
        "\n",
        "* Apply scaling on continuous features and one-hot encoding on categorical features\n",
        "\n",
        "* Separate the features, targets and split the data into train and test\n",
        "\n",
        "* Find the coefficients of the features using normal equation and find the cost (error)\n",
        "\n",
        "* Apply batch gradient descent technique and find the best coefficients\n",
        "\n",
        "* Apply SGD Regressor using sklearn\n",
        "\n",
        "* Apply linear regression using sklearn\n",
        "\n",
        "* Apply Lasso, Ridge, Elasticnet Regression"
      ],
      "id": "opponent-block"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "magnetic-penny"
      },
      "source": [
        "### EDA &  Visualization ( 2 points)"
      ],
      "id": "magnetic-penny"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "constitutional-techno"
      },
      "source": [
        "#### Visualize the hour (hr) column with an appropriate plot and find the busy hours of bike sharing"
      ],
      "id": "constitutional-techno"
    },
    {
      "cell_type": "code",
      "source": [
        "count_grouped_by_hour = df.groupby('hr')['cnt'].sum().reset_index()\n",
        "count_grouped_by_hour"
      ],
      "metadata": {
        "id": "bPlu-cK0aXZf"
      },
      "id": "bPlu-cK0aXZf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist?"
      ],
      "metadata": {
        "id": "9g9i7zrra2Ma"
      },
      "id": "9g9i7zrra2Ma",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_grouped_by_hour.plot(kind=\"bar\",stacked=True, figsize=(10, 8))\n",
        "\n",
        "plt.xlabel(\"Hour\")\n",
        "plt.ylabel(\"Number of Bikers\")\n",
        "plt.title(\"Distribution of Bikers every Hour across all days\")\n",
        "plt.xticks(rotation=0, ha='right')\n",
        "plt.spring()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "INPTTMW7bBJj"
      },
      "id": "INPTTMW7bBJj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It appears that morning 8 AM and evening 5, 6 PM are busiest\n"
      ],
      "metadata": {
        "id": "nK4Po3z-ZgtY"
      },
      "id": "nK4Po3z-ZgtY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flexible-export"
      },
      "source": [
        "#### Visualize the distribution of count, casual and registered variables"
      ],
      "id": "flexible-export"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "manufactured-introduction"
      },
      "source": [
        "# YOUR CODE HERE for distribuiton of count variable\n",
        "plt.hist(df['cnt'], bins=50, edgecolor='black')\n",
        "\n",
        "# Add labels and title\n",
        "plt.title('Histogram of Counts')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "id": "manufactured-introduction",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "powerful-involvement"
      },
      "source": [
        "# YOUR CODE HERE for distribuiton of casual variable\n",
        "plt.hist(df['casual'], bins=50, edgecolor='black')\n",
        "\n",
        "# Add labels and title\n",
        "plt.title('Histogram of Casual Users')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "id": "powerful-involvement",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inside-consideration"
      },
      "source": [
        "# YOUR CODE HERE for distribuiton of registered variable\n",
        "plt.hist(df['registered'], bins=50, edgecolor='black')\n",
        "\n",
        "# Add labels and title\n",
        "plt.title('Histogram of Registered Users')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "id": "inside-consideration",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twelve-burton"
      },
      "source": [
        "#### Describe the relation of weekday, holiday and working day"
      ],
      "id": "twelve-burton"
    },
    {
      "cell_type": "code",
      "source": [
        "df.sample(20)"
      ],
      "metadata": {
        "id": "0m4w400Qi-jA"
      },
      "id": "0m4w400Qi-jA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afraid-proof"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "correlation_matrix = df[['weekday', 'holiday', 'workingday']].corr(method='pearson')\n",
        "correlation_matrix"
      ],
      "id": "afraid-proof",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spearman_corr = df['holiday'].corr(df['workingday'], method='spearman')\n",
        "spearman_corr"
      ],
      "metadata": {
        "id": "nR366BeQj3ZP"
      },
      "id": "nR366BeQj3ZP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thrown-allowance"
      },
      "source": [
        "#### Visualize the month wise count of both casual and registered for the year 2011 and 2012 separately.\n",
        "\n",
        "Hint: Stacked barchart"
      ],
      "id": "thrown-allowance"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "collective-spanking"
      },
      "source": [
        "# stacked bar chart for year 2011\n",
        "# YOUR CODE HERE\n",
        "casual_registered_per_year = df.groupby(['yr', 'mnth']).agg(\n",
        "    total_casual=('casual', 'sum'),\n",
        "    total_registered=('registered', 'sum')\n",
        ").reset_index()\n",
        "casual_registered_per_year"
      ],
      "id": "collective-spanking",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "months = ('1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12')"
      ],
      "metadata": {
        "id": "8ZbbCpUrkkhj"
      },
      "id": "8ZbbCpUrkkhj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stacked_bar_plot(yr: int, casual_users, registered_users):\n",
        "    ind = np.arange(12)    # the 12 months\n",
        "    width = 0.4\n",
        "    p1 = plt.bar(ind, casual_users, width)\n",
        "    p2 = plt.bar(ind, registered_users, width,\n",
        "                bottom=casual_users)\n",
        "    plt.xlabel(f\"Months of {yr}\")\n",
        "    plt.ylabel(\"Number of Users\")\n",
        "    plt.title(f\"Distribution of Casual and Registered Users across 12 months of {yr}\")\n",
        "    plt.xticks(ind, months, rotation=90, ha='right')\n",
        "    plt.legend((p1[0], p2[0]), ('Casual', 'Registered'))\n",
        "    plt.spring()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "l3zQLZz5k4Gj"
      },
      "id": "l3zQLZz5k4Gj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_casual_yr_2011 = casual_registered_per_year.query('yr == 0')['total_casual']\n",
        "total_registered_yr_2011 = casual_registered_per_year.query('yr == 0')['total_registered']\n",
        "stacked_bar_plot('2011', total_casual_yr_2011, total_registered_yr_2011)"
      ],
      "metadata": {
        "id": "CfRZxXQqfUHU"
      },
      "id": "CfRZxXQqfUHU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joined-court"
      },
      "source": [
        "# stacked bar chart for year 2012\n",
        "# YOUR CODE HERE\n",
        "total_casual_yr_2012 = casual_registered_per_year.query('yr == 1')['total_casual']\n",
        "total_registered_yr_2012 = casual_registered_per_year.query('yr == 1')['total_registered']\n",
        "stacked_bar_plot('2012', total_casual_yr_2012, total_registered_yr_2012)"
      ],
      "id": "joined-court",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fifty-driver"
      },
      "source": [
        "#### Analyze the correlation between features with heatmap"
      ],
      "id": "fifty-driver"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "instant-coalition"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "df_new = df.drop('dteday', axis=1)\n",
        "df_new = df_new.apply(pd.to_numeric, errors='coerce')\n",
        "plt.figure(figsize=(14, 14))\n",
        "sns.heatmap(df_new.corr(), annot=True, linewidth=0.5, center=0)\n",
        "plt.show()"
      ],
      "id": "instant-coalition",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It appears that temp and atemp have high correlation. Similarly season and month have high correlations too."
      ],
      "metadata": {
        "id": "iDxXzsW7oms7"
      },
      "id": "iDxXzsW7oms7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pursuant-diary"
      },
      "source": [
        "#### Visualize the box plot of casual and registered variables to check the outliers"
      ],
      "id": "pursuant-diary"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stainless-robert"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "plt.figure(figsize=(10, 6))  # Adjust figure size if needed\n",
        "\n",
        "# Box plot for 'casual'\n",
        "plt.subplot(1, 2, 1)  # 1 row, 2 columns, first subplot\n",
        "sns.boxplot(y=df_new['casual'])\n",
        "plt.title('Box Plot of Casual')\n",
        "plt.ylabel('Casual')\n",
        "\n",
        "# Box plot for 'registered'\n",
        "plt.subplot(1, 2, 2)  # 1 row, 2 columns, second subplot\n",
        "sns.boxplot(y=df_new['registered'])\n",
        "plt.title('Box Plot of Registered')\n",
        "plt.ylabel('Registered')\n",
        "\n",
        "plt.tight_layout()  # Adjust spacing between subplots\n",
        "plt.show()"
      ],
      "id": "stainless-robert",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Casual Median** : 20\n",
        "**Casual Max** : 120\n",
        "\n",
        "**Registered Median** : 120\n",
        "**Registered Max** : ~500\n",
        "\n",
        "Value spread higher for Registered, whereas Outlier Spread higher for casual.\n"
      ],
      "metadata": {
        "id": "glRdcC7mds91"
      },
      "id": "glRdcC7mds91"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "comparative-heritage"
      },
      "source": [
        "### Pre-processing and Data Engineering (1 point)"
      ],
      "id": "comparative-heritage"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "massive-scotland"
      },
      "source": [
        "#### Drop unwanted columns"
      ],
      "id": "massive-scotland"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "appreciated-lawyer"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "# Dropping atemp and windspeed as they do not seem to have any impact on bike sharing\n",
        "df_new = df_new.drop(['atemp', 'windspeed'], axis=1)\n",
        "df_new"
      ],
      "id": "appreciated-lawyer",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nASeXE_7JC0L"
      },
      "source": [
        "#### Identify categorical and continuous variables\n"
      ],
      "id": "nASeXE_7JC0L"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "available-jersey"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "max_unique_for_categorical = 24\n",
        "\n",
        "categorical_columns = []\n",
        "continuous_columns = []\n",
        "\n",
        "[categorical_columns.append(col) if df_new[col].dtype == 'object' or  df[col].nunique() <= max_unique_for_categorical else continuous_columns.append(col) for col in df_new.columns]\n",
        "\n",
        "print(\"Categorical variables:\", categorical_columns)\n",
        "print(\"Continuous variables:\", continuous_columns)"
      ],
      "id": "available-jersey",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "corrected-concrete"
      },
      "source": [
        "#### Feature scaling\n",
        "\n",
        "Feature scaling is essential for machine learning algorithms, the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance. Apply scaling on the continuous variables on the given data.\n",
        "\n",
        "Hint: `MinMaxScaler` or `StandardScaler`\n",
        "\n"
      ],
      "id": "corrected-concrete"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deadly-leisure"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "scaler = StandardScaler()\n",
        "df_scaled = pd.DataFrame(scaler.fit_transform(df_new[continuous_columns]), columns=continuous_columns)\n",
        "df_scaled"
      ],
      "id": "deadly-leisure",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "located-emperor"
      },
      "source": [
        "#### Apply one-hot encode on the categorical data\n",
        "\n",
        "One-hot encoding is applied on the categorical variables, which should not have a different weight or order attached to them, it is presumed that all categorical variables have equivalent \"values\". This means that you cannot simply order them from zero to the number of categories as this would imply that the earlier categories have less \"value\" than later categories.\n",
        "\n",
        "Hint: `sklearn.preprocessing.OneHotEncoder`"
      ],
      "id": "located-emperor"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soviet-stockholm"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "\n",
        "df_one_hot_encoded = encoder.fit_transform(df_new[categorical_columns])\n",
        "\n",
        "# Convert the array to a DataFrame and add column names\n",
        "df_encoded = pd.DataFrame(df_one_hot_encoded, columns=encoder.get_feature_names_out(categorical_columns))\n",
        "# Combine categorical and continuous\n",
        "df_encoded = pd.concat([df_scaled, df_encoded], axis=1)\n",
        "\n",
        "print(f\"Encoded Employee data : \\n{df_encoded}\")"
      ],
      "id": "soviet-stockholm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.get_feature_names_out?"
      ],
      "metadata": {
        "id": "QPxQp0wFke8T"
      },
      "id": "QPxQp0wFke8T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_encoded.columns"
      ],
      "metadata": {
        "id": "QcqWXtJ_fYLZ"
      },
      "id": "QcqWXtJ_fYLZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "straight-teens"
      },
      "source": [
        "#### Specify features and targets after applying scaling and one-hot encoding"
      ],
      "id": "straight-teens"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "civic-private"
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "# YOUR CODE HERE\n",
        "pred_features_columns = ['hr_0', 'hr_1', 'hr_2', 'hr_3', 'hr_4',\n",
        "       'hr_5', 'hr_6', 'hr_7', 'hr_8', 'hr_9', 'hr_10', 'hr_11', 'hr_12',\n",
        "       'hr_13', 'hr_14', 'hr_15', 'hr_16', 'hr_17', 'hr_18', 'hr_19', 'hr_20',\n",
        "       'hr_21', 'hr_22', 'hr_23', 'holiday_0', 'holiday_1', 'hum', 'temp', 'season_1', 'season_2','season_3','season_4','workingday_0','workingday_1','weathersit_1','weathersit_2','weathersit_3','weathersit_4','weekday_0','weekday_1','weekday_2','weekday_3','weekday_4','weekday_5','weekday_6']\n",
        "\n",
        "X = df_encoded[pred_features_columns]\n",
        "# Imputing missing values in X\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X = imputer.fit_transform(X)\n",
        "X = pd.DataFrame(X, columns=pred_features_columns)\n",
        "# Target features is total count, casual and registered users of bikes\n",
        "y = df_encoded[['cnt', 'casual', 'registered']]\n",
        "# YOUR CODE HERE to show 'X'\n",
        "X"
      ],
      "id": "civic-private",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of features selected for prediction {len(pred_features_columns)}\")"
      ],
      "metadata": {
        "id": "fQ-zkhl9gf0Y"
      },
      "id": "fQ-zkhl9gf0Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apparent-restoration"
      },
      "source": [
        "### Implement the linear regression by finding the coefficients using below approaches (2 points)\n",
        "\n",
        "* Find the coefficients using normal equation\n",
        "\n",
        "* (Optional) Implement batch gradient descent\n",
        "\n",
        "* (Optional) SGD Regressor from sklearn"
      ],
      "id": "apparent-restoration"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "involved-shame"
      },
      "source": [
        "#### Select the features and target and split the dataset\n",
        "\n",
        "As there are 3 target variables, choose the count (`cnt`) variable."
      ],
      "id": "involved-shame"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "excess-interview"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "y = df_encoded['cnt']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.25, random_state= 101)"
      ],
      "id": "excess-interview",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.head()"
      ],
      "metadata": {
        "id": "eQAmgnbrhvS-"
      },
      "id": "eQAmgnbrhvS-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA2i6mvbf5yZ"
      },
      "source": [
        "#### Implementation using Normal Equation\n",
        "\n",
        "$\\theta = (X^T X)^{-1} . (X^T Y)$\n",
        "\n",
        "$θ$ is the hypothesis parameter that defines the coefficients\n",
        "\n",
        "$X$ is the input feature value of each instance\n",
        "\n",
        "$Y$ is Output value of each instance\n",
        "\n",
        "For performing Linear Regression Using the Normal Equation refer [here](https://cdn.iisc.talentsprint.com/CDS/Assignments/Module2/M2_SNB_MiniProject_1_LinearRegression_Regularization_Performing%20Linear%20Regression%20using%20Normal%20equation.pdf).\n",
        "\n",
        "To solve the normal equation compute least-squares solution by using `scipy.linalg`\n",
        "\n",
        "Hint: [scipy.linalg.lstsq](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lstsq.html)"
      ],
      "id": "bA2i6mvbf5yZ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66vjMjeHUTGO"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "X = np.array(X_train)\n",
        "Y = np.array(y_train)\n",
        "# Using least squares on training data set.\n",
        "theta_vec, residuals, rank, s = np.linalg.lstsq(X, Y, rcond=None)"
      ],
      "id": "66vjMjeHUTGO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_to_coefficients = {X_train.columns[i]:theta_vec[i] for i in range(len(X_train.columns))}\n",
        "feature_to_coefficients"
      ],
      "metadata": {
        "id": "AlSzzmGIzz3l"
      },
      "id": "AlSzzmGIzz3l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfHHTPUl6joI"
      },
      "source": [
        "#### (Optional) Implementing Linear regression using batch gradient descent\n",
        "\n",
        "Initialize the random coefficients and optimize the coefficients in the iterative process by calculating cost and finding the gradient.\n",
        "\n",
        "Hint: [gradient descent](https://cdn.iisc.talentsprint.com/CDS/Assignments/Module2/M2_SNB_MiniProject_1_LinearRegression_Regularization_Multivariate%20Linear%20Regression.pdf)"
      ],
      "id": "JfHHTPUl6joI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQAmuH2FmXWu"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "def cost_function(X, Y, theta):\n",
        "    m = len(Y)\n",
        "    J = np.sum((X.dot(theta) - Y) ** 2) / 2 * m\n",
        "    return J"
      ],
      "id": "vQAmuH2FmXWu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_gradient_descent(X, Y, theta, alpha, iterations):\n",
        "    cost_history = [0] * iterations\n",
        "    m = len(Y)\n",
        "\n",
        "    for iter in range(iterations):\n",
        "        # Calculate hypothesis values\n",
        "        h = X.dot(theta)\n",
        "        # Difference between actual and calculated\n",
        "        loss = h - Y\n",
        "        # Gradient Calculation\n",
        "        gradient = X.T.dot(loss) / m\n",
        "        theta -= alpha * gradient\n",
        "        cost = cost_function(X, Y, theta)\n",
        "        cost_history[iter] = cost\n",
        "    return theta, cost_history\n"
      ],
      "metadata": {
        "id": "fkDyuYsg38f7"
      },
      "id": "fkDyuYsg38f7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "errs_1 = []\n",
        "theta = np.zeros(X_train.shape[1])         # initial guess\n",
        "alpha = 0.05                               # learning rate\n",
        "iterations = 2000\n",
        "\n",
        "final_theta, cost_history = batch_gradient_descent(X_train, y_train, theta, alpha, iterations)\n",
        "print(f\"Theta values final {final_theta}\")\n",
        "print(f\"Cost History {cost_history}\")"
      ],
      "metadata": {
        "id": "oGq3Zk3gXAki"
      },
      "id": "oGq3Zk3gXAki",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmZJdg33NlvA"
      },
      "source": [
        "#### (Optional) SGD Regressor\n",
        "\n",
        "Scikit-learn API provides the SGDRegressor class to implement SGD method for regression problems. The SGD regressor applies regularized linear model with SGD learning to build an estimator. A regularizer is a penalty (L1, L2, or Elastic Net) added to the loss function to shrink the model parameters.\n",
        "\n",
        "* Import SGDRegressor from sklearn and fit the data\n",
        "\n",
        "* Predict the test data and find the error\n",
        "\n",
        "Hint: [SGDRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html)"
      ],
      "id": "PmZJdg33NlvA"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fU1-w4XRNlLA"
      },
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "# YOUR CODE HERE\n",
        "sgd = SGDRegressor(max_iter=2000, tol=1e-3, penalty='l2', random_state=42)\n",
        "\n",
        "# Fit the model on the training data\n",
        "sgd.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = sgd.predict(X_test)\n",
        "\n",
        "# Calculate the mean squared error of the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"Intercept:\", sgd.intercept_)"
      ],
      "id": "fU1-w4XRNlLA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sgd_features_to_coefficients = {X_train.columns[i]:theta_vec[i] for i in range(len(X_train.columns))}\n",
        "sgd_features_to_coefficients"
      ],
      "metadata": {
        "id": "TVZr6MQH150d"
      },
      "id": "TVZr6MQH150d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "legal-engagement"
      },
      "source": [
        "### Linear regression using sklearn (3 points)\n",
        "\n",
        "Implement the linear regression model using sklearn\n",
        "\n",
        "* Import Linear Regression and fit the train data\n",
        "\n",
        "* Predict the test data and find the error\n",
        "\n",
        "Hint: [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)"
      ],
      "id": "legal-engagement"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "complicated-reserve"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "lr = LinearRegression()\n",
        "# Fit lin reg model to train\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "pred = lr.predict(X_test)\n",
        "print('Mean Absolute Error:', mean_absolute_error(y_test, pred))\n",
        "print('Mean Squared Error:', mean_squared_error(y_test, pred))\n",
        "print('Mean Root Squared Error:', np.sqrt(mean_squared_error(y_test, pred)))"
      ],
      "id": "complicated-reserve",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quality-morgan"
      },
      "source": [
        "#### Calculate the $R^2$ (coefficient of determination) of the actual and predicted data"
      ],
      "id": "quality-morgan"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "important-jacket"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "print('Coefficient of Determination:', r2_score(y_test, pred))"
      ],
      "id": "important-jacket",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr."
      ],
      "metadata": {
        "id": "p9fCtXpP4McE"
      },
      "id": "p9fCtXpP4McE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "infinite-victim"
      },
      "source": [
        "#### Summarize the importance of features\n",
        "\n",
        "Prediction is the weighted sum of the input values e.g. linear regression. Regularization, such as ridge regression and the elastic net, find a set of coefficients to use in the weighted sum to make a prediction. These coefficients can be used directly as a crude type of feature importance score.\n",
        "This assumes that the input variables have the same scale or have been scaled prior to fitting a model.\n",
        "\n",
        "Use the coefficients obtained through the sklearn Linear Regression implementation and create a bar chart of the coefficients."
      ],
      "id": "infinite-victim"
    },
    {
      "cell_type": "code",
      "source": [
        "df_coefficients = pd.DataFrame({'Features': X_train.columns, 'Coefficients': lr.coef_})\n",
        "df_coefficients"
      ],
      "metadata": {
        "id": "qB2tK4cM4Z0f"
      },
      "id": "qB2tK4cM4Z0f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "affected-walker"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "num_features = np.arange(df_coefficients.shape[0])\n",
        "df_coefficients.plot(kind=\"bar\",stacked=True, figsize=(14, 8))\n",
        "\n",
        "plt.xlabel(\"Features\")\n",
        "plt.ylabel(\"Coefficients\")\n",
        "plt.title(\"Distribution of Bikers every Hour across all days\")\n",
        "plt.xticks(num_features, df_coefficients['Features'], rotation=90, ha='right')\n",
        "plt.spring()\n",
        "plt.show()\n"
      ],
      "id": "affected-walker",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "convinced-snowboard"
      },
      "source": [
        "### Regularization methods (2 points)"
      ],
      "id": "convinced-snowboard"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twenty-italic"
      },
      "source": [
        "#### Apply Lasso regression\n",
        "\n",
        "* Apply Lasso regression with different alpha values given below and find the best alpha that gives the least error.\n",
        "* Calculate the metrics for the actual and predicted\n",
        "\n",
        "Hint: [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)"
      ],
      "id": "twenty-italic"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psychological-blake"
      },
      "source": [
        "# setting up alpha\n",
        "alphas = [0.0001, 0.001,0.01, 0.1, 1, 10, 100]"
      ],
      "id": "psychological-blake",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "existing-sigma"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "coefs = []\n",
        "least_error_value = -1\n",
        "alpha_for_least_error = tuple()\n",
        "# Loop over each alpha and fit a Lasso model\n",
        "for alpha in alphas:\n",
        "    lasso = Lasso(alpha=alpha)\n",
        "    lasso.fit(X_train, y_train)\n",
        "    pred = lasso.predict(X_test)\n",
        "    error = mean_squared_error(y_test, pred)\n",
        "\n",
        "    if least_error_value < 0:\n",
        "        least_error_value = error\n",
        "        alpha_for_least_error = (alpha, least_error_value)\n",
        "    else:\n",
        "        if error < least_error_value:\n",
        "            least_error_value = error\n",
        "            alpha_for_least_error = (alpha, least_error_value)\n",
        "\n",
        "\n",
        "print(f'Least Mean Squared Error: {alpha_for_least_error[1]} observed for alpha {alpha_for_least_error[0]}')"
      ],
      "id": "existing-sigma",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "figured-effectiveness"
      },
      "source": [
        "#### Apply Ridge regression\n",
        "\n",
        "* Apply Ridge regression with different alpha values given and find the best alpha that gives the least error.\n",
        "* Calculate the metrics for the actual and predicted\n",
        "\n",
        "Hint: [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)"
      ],
      "id": "figured-effectiveness"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "curious-initial"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "for alpha in alphas:\n",
        "    rdg = linear_model.Ridge(alpha = alpha)        # instantiate Ridge regressor\n",
        "    # YOUR CODE HERE to fit(x, y) on 'rdg'\n",
        "    rdg.fit(X_train, y_train)\n",
        "    pred = rdg.predict(X_test)\n",
        "    error = mean_squared_error(y_test, pred)\n",
        "\n",
        "    if least_error_value < 0:\n",
        "        least_error_value = error\n",
        "        alpha_for_least_error = (alpha, least_error_value)\n",
        "    else:\n",
        "        if error < least_error_value:\n",
        "            least_error_value = error\n",
        "            alpha_for_least_error = (alpha, least_error_value)\n",
        "\n",
        "\n",
        "print(f'Least Mean Squared Error: {alpha_for_least_error[1]} observed for alpha {alpha_for_least_error[0]}')"
      ],
      "id": "curious-initial",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exposed-bottom"
      },
      "source": [
        "#### Apply Elasticnet regression\n",
        "\n",
        "* Apply Elasticnet regression with different alpha values given and find the best alpha that gives the least error.\n",
        "* Calculate the metrics for the actual and predicted\n",
        "\n",
        "Hint: [ElasticNet](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html)"
      ],
      "id": "exposed-bottom"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shared-belief"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "for alpha in alphas:\n",
        "    en = linear_model.ElasticNet(alpha = alpha, random_state = 0, max_iter=1000)        # instantiate ElasticNet regressor\n",
        "    # YOUR CODE HERE to fit(x, y) on 'en'\n",
        "    en.fit(X_train, y_train)\n",
        "    pred = en.predict(X_test)\n",
        "    error = mean_squared_error(y_test, pred)\n",
        "\n",
        "    if least_error_value < 0:\n",
        "        least_error_value = error\n",
        "        alpha_for_least_error = (alpha, least_error_value)\n",
        "    else:\n",
        "        if error < least_error_value:\n",
        "            least_error_value = error\n",
        "            alpha_for_least_error = (alpha, least_error_value)\n",
        "\n",
        "\n",
        "print(f'Least Mean Squared Error: {alpha_for_least_error[1]} observed for alpha {alpha_for_least_error[0]}')"
      ],
      "id": "shared-belief",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6bc_VBGEG7C"
      },
      "source": [
        "### Determine if there is a reduction in error if two target variables are considered\n",
        "\n",
        "Consider (`Casual, Registered`) as target and find the error by implementing Linear Regression model from sklearn"
      ],
      "id": "B6bc_VBGEG7C"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5l5ZviEMy-o"
      },
      "source": [
        "### Report Analysis\n",
        "\n",
        "* Describe your interpretation of the methods that are used to implement linear regression covered in this mini project.\n",
        "* Comment on performance of the algorithms/methods used.\n",
        "* Comment about the nature of the data and fitment of linear regression for this data.\n",
        "* Can you perform a non linear curve fitting using linear regression? If yes, How?\n"
      ],
      "id": "i5l5ZviEMy-o"
    }
  ]
}